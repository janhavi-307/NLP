{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Dtp9m6YNUnNU"
      },
      "outputs": [],
      "source": [
        "#Perform bag-of-words approach (count occurrence, normalized count occurrence)\n",
        "#TF-IDF on data.\n",
        "#Create embeddings using Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy pandas scikit-learn gensim\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F81cq-HWUvGQ",
        "outputId": "4e6c3279-f789-40fe-8318-89a70dfde057"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        ""
      ],
      "metadata": {
        "id": "tewxgWpTU3Qe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AkWlLGuU7N6",
        "outputId": "54246351-5f3a-43ce-87fc-982583dbbd11"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text data (Replace this with your dataset)\n",
        "documents = [\n",
        "    \"AI is transforming the future.\",\n",
        "    \"Machine learning is a part of AI.\",\n",
        "    \"Natural language processing is useful for AI applications.\"\n",
        "]\n",
        "\n"
      ],
      "metadata": {
        "id": "Cwd18kP5VFN0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to Pandas DataFrame\n",
        "df = pd.DataFrame(documents, columns=[\"Text\"])\n",
        ""
      ],
      "metadata": {
        "id": "dco05KuuVHAN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer()\n",
        "bow_matrix = vectorizer.fit_transform(df[\"Text\"])\n",
        ""
      ],
      "metadata": {
        "id": "tLnyw6fDVLZF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to DataFrame for better visualization\n",
        "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "print(\"Bag-of-Words (Raw Count):\")\n",
        "print(bow_df)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kziuBSSWVPSP",
        "outputId": "090a57ec-9a54-4e36-c56a-f2749659097e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag-of-Words (Raw Count):\n",
            "   ai  applications  for  future  is  language  learning  machine  natural  \\\n",
            "0   1             0    0       1   1         0         0        0        0   \n",
            "1   1             0    0       0   1         0         1        1        0   \n",
            "2   1             1    1       0   1         1         0        0        1   \n",
            "\n",
            "   of  part  processing  the  transforming  useful  \n",
            "0   0     0           0    1             1       0  \n",
            "1   1     1           0    0             0       0  \n",
            "2   0     0           1    0             0       1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import normalize\n",
        "\n"
      ],
      "metadata": {
        "id": "vtiMidvLVTUH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_bow = normalize(bow_matrix, norm='l1', axis=1)  # L1 Normalization\n",
        "# Convert sparse matrix to dense array to solve shape mismatch.\n",
        "normalized_bow_dense = normalized_bow.toarray()\n",
        "normalized_bow_df = pd.DataFrame(normalized_bow_dense, columns=vectorizer.get_feature_names_out())\n",
        "print(\"\\nBag-of-Words (Normalized Count):\")\n",
        "print(normalized_bow_df)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsyN2yW3VXRT",
        "outputId": "1f66eda8-9678-4320-d482-4c4a7f0969f7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Bag-of-Words (Normalized Count):\n",
            "         ai  applications    for  future        is  language  learning  \\\n",
            "0  0.200000         0.000  0.000     0.2  0.200000     0.000  0.000000   \n",
            "1  0.166667         0.000  0.000     0.0  0.166667     0.000  0.166667   \n",
            "2  0.125000         0.125  0.125     0.0  0.125000     0.125  0.000000   \n",
            "\n",
            "    machine  natural        of      part  processing  the  transforming  \\\n",
            "0  0.000000    0.000  0.000000  0.000000       0.000  0.2           0.2   \n",
            "1  0.166667    0.000  0.166667  0.166667       0.000  0.0           0.0   \n",
            "2  0.000000    0.125  0.000000  0.000000       0.125  0.0           0.0   \n",
            "\n",
            "   useful  \n",
            "0   0.000  \n",
            "1   0.000  \n",
            "2   0.125  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and fit TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer() # Initialize the TfidfVectorizer\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df[\"Text\"]) # Fit and transform\n",
        ""
      ],
      "metadata": {
        "id": "KMvXIoeMW8am"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to DataFrame for better visualization\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "print(\"\\nTF-IDF Representation:\")\n",
        "print(tfidf_df)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfHeThyeXBDd",
        "outputId": "f1e97992-efb1-460e-cfb9-e528dbb43a75"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TF-IDF Representation:\n",
            "         ai  applications       for   future        is  language  learning  \\\n",
            "0  0.307144      0.000000  0.000000  0.52004  0.307144  0.000000  0.000000   \n",
            "1  0.272499      0.000000  0.000000  0.00000  0.272499  0.000000  0.461381   \n",
            "2  0.228215      0.386401  0.386401  0.00000  0.228215  0.386401  0.000000   \n",
            "\n",
            "    machine   natural        of      part  processing      the  transforming  \\\n",
            "0  0.000000  0.000000  0.000000  0.000000    0.000000  0.52004       0.52004   \n",
            "1  0.461381  0.000000  0.461381  0.461381    0.000000  0.00000       0.00000   \n",
            "2  0.000000  0.386401  0.000000  0.000000    0.386401  0.00000       0.00000   \n",
            "\n",
            "     useful  \n",
            "0  0.000000  \n",
            "1  0.000000  \n",
            "2  0.386401  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the 'punkt_tab' resource:\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGu09syRXPjn",
        "outputId": "ff600646-d4e5-427a-e259-3fc77bb27069"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing the sentences for Word2Vec\n",
        "tokenized_text = [word_tokenize(doc.lower()) for doc in df[\"Text\"]]\n",
        ""
      ],
      "metadata": {
        "id": "mhxvvpQiXkWj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Word2Vec model\n",
        "word2vec_model = Word2Vec(sentences=tokenized_text, vector_size=100, window=5, min_count=1, workers=4)\n",
        ""
      ],
      "metadata": {
        "id": "iM2Qb3YJXp1T"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get vector for a word\n",
        "word = \"ai\"\n",
        "if word in word2vec_model.wv:\n",
        "    print(f\"\\nWord2Vec vector for '{word}':\")\n",
        "    print(word2vec_model.wv[word])\n",
        "else:\n",
        "    print(f\"\\nWord '{word}' not found in Word2Vec vocabulary.\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZS-JYB22Xvjq",
        "outputId": "89badaea-ed8a-432d-a1ae-8e8f4b169f70"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word2Vec vector for 'ai':\n",
            "[ 9.4563962e-05  3.0773198e-03 -6.8126451e-03 -1.3754654e-03\n",
            "  7.6685809e-03  7.3464094e-03 -3.6732971e-03  2.6427018e-03\n",
            " -8.3171297e-03  6.2054861e-03 -4.6373224e-03 -3.1641065e-03\n",
            "  9.3113566e-03  8.7338570e-04  7.4907029e-03 -6.0740625e-03\n",
            "  5.1605068e-03  9.9228229e-03 -8.4573915e-03 -5.1356913e-03\n",
            " -7.0648370e-03 -4.8626517e-03 -3.7785638e-03 -8.5361991e-03\n",
            "  7.9556061e-03 -4.8439382e-03  8.4236134e-03  5.2625705e-03\n",
            " -6.5500261e-03  3.9578713e-03  5.4701497e-03 -7.4265362e-03\n",
            " -7.4057197e-03 -2.4752307e-03 -8.6257253e-03 -1.5815723e-03\n",
            " -4.0343284e-04  3.2996845e-03  1.4418805e-03 -8.8142155e-04\n",
            " -5.5940580e-03  1.7303658e-03 -8.9737179e-04  6.7936908e-03\n",
            "  3.9735902e-03  4.5294715e-03  1.4343059e-03 -2.6998555e-03\n",
            " -4.3668128e-03 -1.0320747e-03  1.4370275e-03 -2.6460087e-03\n",
            " -7.0737829e-03 -7.8053069e-03 -9.1217868e-03 -5.9351693e-03\n",
            " -1.8474245e-03 -4.3238713e-03 -6.4606704e-03 -3.7173224e-03\n",
            "  4.2891586e-03 -3.7390434e-03  8.3781751e-03  1.5339935e-03\n",
            " -7.2423196e-03  9.4337985e-03  7.6312125e-03  5.4932819e-03\n",
            " -6.8488456e-03  5.8226790e-03  4.0090932e-03  5.1853694e-03\n",
            "  4.2559016e-03  1.9397545e-03 -3.1701624e-03  8.3538452e-03\n",
            "  9.6121803e-03  3.7926030e-03 -2.8369951e-03  7.1275235e-06\n",
            "  1.2188185e-03 -8.4583247e-03 -8.2239453e-03 -2.3101569e-04\n",
            "  1.2372875e-03 -5.7433806e-03 -4.7252737e-03 -7.3460746e-03\n",
            "  8.3286157e-03  1.2129784e-04 -4.5093987e-03  5.7017053e-03\n",
            "  9.1800150e-03 -4.0998720e-03  7.9646818e-03  5.3754342e-03\n",
            "  5.8791232e-03  5.1259040e-04  8.2130842e-03 -7.0190406e-03]\n"
          ]
        }
      ]
    }
  ]
}